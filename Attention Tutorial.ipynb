{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1·# Attention Tutorial\n",
    "\n",
    "One of the most influential and interesting new neural networks types is the attention network. It's been used succesfully in translation services, [medical diagnosis](https://arxiv.org/pdf/1710.08312.pdf), and other tasks.\n",
    "\n",
    "Below we'll be walking through how to write your very own attention network. Our goal is to make a network that can translate human written times ('quarter after 3 pm') to military time ('15:15').\n",
    "\n",
    "The attention mechamism is defined in section **Model**.\n",
    "\n",
    "For a tutorial on how Attention Networks work, please visit [MuffinTech](http://muffintech.org/blog/id/12).\n",
    "\n",
    "Credit to Andrew Ng for reference model and inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "以[\"six hours and fifty five am\",\"06:55\"]实例为例进行模型分析\n",
    "问题定义：\n",
    "将人类语言描述的时间，记为X；将标准数字描述的时间，\n",
    "记为Y。即<X,Y>类型，符合Encoder-Decoder框架。 \n",
    "X=[\"six hours and fifty five am\"]，Y=[\"06:55\"]    \n",
    "任务：将X通过模型转换成Y\n",
    "'''\n",
    "# Imports\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Reshape\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Pinkie Pie was here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset was created using some simple rules. It is not exhaustive, but provides some very nice challenges.\n",
    "\n",
    "The dataset is included in the Github repo.\n",
    "\n",
    "Some example data pairs are listed below:\n",
    "\n",
    "['48 min before 10 a.m', '09:12']  \n",
    "['t11:36', '11:36']  \n",
    "[\"nine o'clock forty six p.m\", '21:46']  \n",
    "['2:59p.m.', '14:59']  \n",
    "['23 min after 20 p.m.', '20:23']  \n",
    "['46 min after seven p.m.', '19:46']  \n",
    "['10 before nine pm', '20:50']  \n",
    "['3.20', '03:20']  \n",
    "['7.57', '07:57']  \n",
    "['six hours and fifty five am', '06:55']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集\n",
    "with open('data/Time Dataset.json','r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "with open('data/Time Vocabs.json','r') as f:\n",
    "    human_vocab, machine_vocab = json.loads(f.read())\n",
    "    \n",
    "human_vocab_size = len(human_vocab)\n",
    "machine_vocab_size = len(machine_vocab)\n",
    "\n",
    "# Number of training examples\n",
    "m = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define some general helper methods. They are used to help tokenize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"\n",
    "    A method for tokenizing data.\n",
    "    \n",
    "    Inputs:\n",
    "    dataset - A list of sentence data pairs.\n",
    "    human_vocab - A dictionary of tokens (char) to id's.\n",
    "    machine_vocab - A dictionary of tokens (char) to id's.\n",
    "    Tx - X data size\n",
    "    Ty - Y data size\n",
    "    \n",
    "    Outputs:\n",
    "    X - Sparse tokens for X data\n",
    "    Y - Sparse tokens for Y data\n",
    "    Xoh - One hot tokens for X data\n",
    "    Yoh - One hot tokens for Y data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metadata\n",
    "    m = len(dataset)\n",
    "    \n",
    "    # Initialize\n",
    "    X = np.zeros([m, Tx], dtype='int32')\n",
    "    Y = np.zeros([m, Ty], dtype='int32')\n",
    "    \n",
    "    # Process data\n",
    "    for i in range(m):\n",
    "        data = dataset[i]\n",
    "        X[i] = np.array(tokenize(data[0], human_vocab, Tx))\n",
    "        Y[i] = np.array(tokenize(data[1], machine_vocab, Ty))\n",
    "    \n",
    "    # Expand one hots\n",
    "    Xoh = oh_2d(X, len(human_vocab))\n",
    "    Yoh = oh_2d(Y, len(machine_vocab))\n",
    "    \n",
    "    return (X, Y, Xoh, Yoh)\n",
    "    \n",
    "def tokenize(sentence, vocab, length):\n",
    "    \"\"\"\n",
    "    Returns a series of id's for a given input token sequence.\n",
    "    \n",
    "    It is advised that the vocab supports <pad> and <unk>.\n",
    "    \n",
    "    Inputs:\n",
    "    sentence - Series of tokens\n",
    "    vocab - A dictionary from token to id\n",
    "    length - Max number of tokens to consider\n",
    "    \n",
    "    Outputs:\n",
    "    tokens - \n",
    "    \"\"\"\n",
    "    tokens = [0]*length\n",
    "    for i in range(length):\n",
    "        char = sentence[i] if i < len(sentence) else \"<pad>\"\n",
    "        char = char if (char in vocab) else \"<unk>\"\n",
    "        tokens[i] = vocab[char]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def ids_to_keys(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a series of id's into the keys of a dictionary.\n",
    "    \"\"\"\n",
    "    return [list(vocab.keys())[id] for id in sentence]\n",
    "\n",
    "def oh_2d(dense, max_value):\n",
    "    \"\"\"\n",
    "    Create a one hot array for the 2D input dense array.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    oh = np.zeros(np.append(dense.shape, [max_value]))\n",
    "    \n",
    "    # Set correct indices\n",
    "    ids1, ids2 = np.meshgrid(np.arange(dense.shape[0]), np.arange(dense.shape[1]))\n",
    "    \n",
    "    oh[ids1.flatten(), ids2.flatten(), dense.flatten('F').astype(int)] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal is to tokenize the data using our vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "将X、Y数据处理成索引形式，每一个索引对应于一个one-hot向量\n",
    "example:\n",
    "X=\"six hours and fifty five am\"   \n",
    "len(X)=27,\n",
    "模型中设置了X数据中的最大长度为41，索引len(X)=27<41，得进行索引填充(padding)。\n",
    "X=['s','i','x',' ','h','o','u','r','s',' ','a','n','d',' ','f','i','f','t','y',' ','f','i','v','e',' ','a','m']\n",
    "去字典human_vocab查询其索引值，并填充到41的长度，\n",
    "于是：索引X=[31 22 36  0 21 27 33 30 31  0 14 26 17  0 19 22 19 32 37  0 19 22 34 18  0 14 25 40 40 40 40 40 40 40 40 40 40 40 40 40 40]\n",
    "同理，Y=\"06:55\"，len(Y)=5，Y数据集的长度都为5，所以不需要填充。\n",
    "'''\n",
    "Tx = 41 # X集合可以由41个不同的字符构成，Max x sequence length\n",
    "Ty = 5 # Y数据集的长度是5\n",
    "'''\n",
    "将X索引转换为Xoh(one-hot)形式，Xoh维度:(41x41)；将Y索引转换为Yoh(one-hot)形式，Yoh维度:(5x11)\n",
    "'''\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "# Split data 80-20 between training and test\n",
    "\n",
    "train_size = int(0.8*m)\n",
    "Xoh_train = Xoh[:train_size]\n",
    "Yoh_train = Yoh[:train_size]\n",
    "Xoh_test = Xoh[train_size:]\n",
    "Yoh_test = Yoh[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be careful, let's check that the code works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data point 4.\n",
      "\n",
      "The data input is: 8:25\n",
      "The data output is: 08:25\n",
      "\n",
      "The tokenized input is:[11 13  5  8 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40]\n",
      "The tokenized output is: [ 0  8 10  2  5]\n",
      "\n",
      "The one-hot input is: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "The one-hot output is: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "print(\"Input data point \" + str(i) + \".\")\n",
    "print(\"\")\n",
    "print(\"The data input is: \" + str(dataset[i][0]))\n",
    "print(\"The data output is: \" + str(dataset[i][1]))\n",
    "print(\"\")\n",
    "print(\"The tokenized input is:\" + str(X[i]))\n",
    "print(\"The tokenized output is: \" + str(Y[i]))\n",
    "print(\"\")\n",
    "print(\"The one-hot input is:\", Xoh[i])\n",
    "print(\"The one-hot output is:\", Yoh[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our next goal is to define our model. The important part will be defining the attention mechanism and then making sure to apply that correctly.\n",
    "\n",
    "Define some model metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "模型搭建\n",
    "'''\n",
    "layer1_size = 32\n",
    "layer2_size = 64 # Attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two code snippets defined the attention mechanism. This is split into two arcs:\n",
    "\n",
    "* Calculating context\n",
    "* Creating an attention layer\n",
    "\n",
    "As a refresher, an attention network pays attention to certain parts of the input at each output time step. _attention_ denotes which inputs are most relevant to the current output step. An input step will have attention weight ~1 if it is relevant, and ~0 otherwise. The _context_ is the \"summary of the input\".\n",
    "\n",
    "The requirements are thus. The attention matrix should have shape $(T_x)$ and sum to 1. Additionally, the context should be calculated in the same manner for each time step. Beyond that, there is some flexibility. This notebook calculates both this way:\n",
    "\n",
    "$$\n",
    "attention = Softmax(Dense(Dense(x, y_{t-1})))\n",
    "$$\n",
    "<br/>\n",
    "$$\n",
    "context = \\sum_{i=1}^{m} ( attention_i * x_i )\n",
    "$$\n",
    "\n",
    "For safety, $y_0$ is defined as $\\vec{0}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define part of the attention layer gloablly so as to\n",
    "# share the same layers for each attention step.\n",
    "def softmax(x):\n",
    "    return K.softmax(x, axis=1)\n",
    "'''\n",
    "one_step_of_attention()每一步获取注意力的过程\n",
    "'''\n",
    "#以下设置一些默认参数\n",
    "at_repeat = RepeatVector(Tx)#Tx=41  RepeatVector对向量进行重复\n",
    "at_concatenate = Concatenate(axis=-1)#Concatenate接收一个列表的同shape张量\n",
    "at_dense1 = Dense(8, activation=\"tanh\")#全连接层\n",
    "at_dense2 = Dense(1, activation=\"relu\")#全连接层\n",
    "at_softmax = Activation(softmax, name='attention_weights')#获取注意力权重\n",
    "at_dot = Dot(axes=1)#计算乘积\n",
    "\n",
    "\n",
    "#获取注意力\n",
    "def one_step_of_attention(h_prev, a):\n",
    "    \"\"\"\n",
    "    #h是lambda层处理后的中间语义；X是a1,是Encoder得到的中间语义\n",
    "    #h_prev=h;a=X\n",
    "    \"\"\"\n",
    "    # Repeat vector to match a's dimensions\n",
    "    #对向量h_prev进行重复，会重复Tx次，这样就能匹配上a的维度\n",
    "    h_repeat = at_repeat(h_prev)\n",
    "    # Calculate attention weights\n",
    "    #计算注意力权重\n",
    "    i = at_concatenate([a, h_repeat])#将a和h_repeat进行拼接\n",
    "    #使用两层全连接网络\n",
    "    i = at_dense1(i)\n",
    "    i = at_dense2(i)\n",
    "    #用softmax（）归一化处理\n",
    "    attention = at_softmax(i)\n",
    "    # Calculate the context\n",
    "    #得到文本注意力权重\n",
    "    context = at_dot([attention, a])#将注意力权值与a进行乘积得到整个文本注意力后的信息\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "获得注意力的实现\n",
    "'''\n",
    "def attention_layer(X, n_h, Ty):\n",
    "    \"\"\"\n",
    "    #a1是encoder得到的中间语义，layer2_size=64,Ty=5\n",
    "    #X=a1;n_h=layer2_size=64;Ty=5\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Define the default state for the LSTM layer\n",
    "    #将Encoder得到的中间语义X数据进行变换，变换本身没有什么需要学习的参数，所以使用lamda层\n",
    "    #X：m*41*64 --> h:m*64\n",
    "    #X：m*41*64 --> c:m*64\n",
    "    h = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)))(X)\n",
    "    c = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)))(X)\n",
    "    # Messy, but the alternative is using more Input()\n",
    "    \n",
    "    at_LSTM = LSTM(n_h, return_state=True)\n",
    "    \n",
    "    output = []\n",
    "              \n",
    "    # Run attention step and RNN for each output time step\n",
    "    for _ in range(Ty):\n",
    "        #对y的每一步预测，都计算一下注意力，接下来跳转到one_step_of_attention(h_prev, a)函数\n",
    "        context = one_step_of_attention(h, X)\n",
    "        \n",
    "        #context是通过注意力模型得到的每个文本注意力权值计算结果\n",
    "        #输入初始化的h和c，然后不断的改变h和c\n",
    "        h, _, c = at_LSTM(context, initial_state=[h, c])\n",
    "        \n",
    "        output.append(h)#返回每一步注意力模型计算得到的h\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample model is organized as follows:\n",
    "\n",
    "1. BiLSTM\n",
    "2. Attention Layer\n",
    "    * Outputs Ty lists of activations.\n",
    "3. Dense\n",
    "    * Necessary to convert attention layer's output to the correct y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3 = Dense(machine_vocab_size, activation=softmax)\n",
    "'''\n",
    "通过get_model()函数获得搭建的模型\n",
    "'''\n",
    "\n",
    "def get_model(Tx, Ty, layer1_size, layer2_size, x_vocab_size, y_vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a model.\n",
    "    \n",
    "    input:\n",
    "    Tx - Number of x timesteps\n",
    "    Ty - Number of y timesteps\n",
    "    size_layer1 - Number of neurons in BiLSTM\n",
    "    size_layer2 - Number of neurons in attention LSTM hidden layer\n",
    "    x_vocab_size - Number of possible token types for x\n",
    "    y_vocab_size - Number of possible token types for y\n",
    "    \n",
    "    Output:\n",
    "    model - A Keras Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create layers one by one\n",
    "    #输入X：（1*41*41） 假设只有一个实例\n",
    "    X = Input(shape=(Tx, x_vocab_size))\n",
    "    #双向LSTM作为encoder编码层 接收X作为输入，输出a1:(1*41*64)\n",
    "    a1 = Bidirectional(LSTM(layer1_size, return_sequences=True), merge_mode='concat')(X)\n",
    "    #使用注意力层对a1处理\n",
    "    a2 = attention_layer(a1, layer2_size, Ty)\n",
    "    #将a2每一步得到的计算值，输入到layer3这个解码器中，这个解码器就是一个全连接层\n",
    "    a3 = [layer3(timestep) for timestep in a2]#layer3是Dense全连接层，作为decoder解码层\n",
    "        \n",
    "    # Create Keras model\n",
    "    model = Model(inputs=[X], outputs=a3)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps from here on out are for creating the model and training it. Simple as that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#获取搭建模型的样子 注意：必须用pip安装pydot和graphviz   \n",
    "\n",
    "from keras.utils.vis_utils import plot_model  #方便打印模型图片\n",
    "# Obtain a model instance\n",
    "model = get_model(Tx, Ty, layer1_size, layer2_size, human_vocab_size, machine_vocab_size)\n",
    "#保存模型图片\n",
    "plot_model(model,to_file='attention.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "opt = Adam(lr=0.05, decay=0.04, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])#categorical_crossentropy多分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the output by timestep, not example\n",
    "#Yoh_train是(5*11维)，swapaxes交换得(11*5维)\n",
    "outputs_train = list(Yoh_train.swapaxes(0,1))#output_train为11*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 7.6577 - dense_3_loss: 2.3444 - dense_3_acc: 0.5070 - dense_3_acc_1: 0.1709 - dense_3_acc_2: 0.9625 - dense_3_acc_3: 0.1943 - dense_3_acc_4: 0.1104\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 6s 697us/step - loss: 5.7468 - dense_3_loss: 2.1015 - dense_3_acc: 0.7647 - dense_3_acc_1: 0.4358 - dense_3_acc_2: 0.9998 - dense_3_acc_3: 0.3314 - dense_3_acc_4: 0.2140\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 6s 735us/step - loss: 4.3258 - dense_3_loss: 1.7012 - dense_3_acc: 0.8457 - dense_3_acc_1: 0.6656 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.4745 - dense_3_acc_4: 0.3726\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 6s 755us/step - loss: 3.3489 - dense_3_loss: 1.3558 - dense_3_acc: 0.9053 - dense_3_acc_1: 0.7787 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.5911 - dense_3_acc_4: 0.5125\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 7s 817us/step - loss: 2.6863 - dense_3_loss: 1.0698 - dense_3_acc: 0.9234 - dense_3_acc_1: 0.8297 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.6746 - dense_3_acc_4: 0.6234\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 6s 765us/step - loss: 2.2014 - dense_3_loss: 0.8342 - dense_3_acc: 0.9383 - dense_3_acc_1: 0.8628 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.7326 - dense_3_acc_4: 0.7142\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.8481 - dense_3_loss: 0.6604 - dense_3_acc: 0.9495 - dense_3_acc_1: 0.8884 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.7694 - dense_3_acc_4: 0.7809\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 1.5891 - dense_3_loss: 0.5393 - dense_3_acc: 0.9540 - dense_3_acc_1: 0.9041 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.7914 - dense_3_acc_4: 0.8300\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.3727 - dense_3_loss: 0.4415 - dense_3_acc: 0.9621 - dense_3_acc_1: 0.9220 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8196 - dense_3_acc_4: 0.8625\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.2126 - dense_3_loss: 0.3745 - dense_3_acc: 0.9683 - dense_3_acc_1: 0.9313 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8360 - dense_3_acc_4: 0.8911\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 6s 739us/step - loss: 1.0900 - dense_3_loss: 0.3193 - dense_3_acc: 0.9711 - dense_3_acc_1: 0.9400 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8472 - dense_3_acc_4: 0.9069\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 6s 714us/step - loss: 0.9842 - dense_3_loss: 0.2807 - dense_3_acc: 0.9756 - dense_3_acc_1: 0.9455 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8589 - dense_3_acc_4: 0.9216\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 6s 722us/step - loss: 0.8925 - dense_3_loss: 0.2444 - dense_3_acc: 0.9799 - dense_3_acc_1: 0.9533 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8710 - dense_3_acc_4: 0.9348\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 6s 700us/step - loss: 0.8071 - dense_3_loss: 0.2147 - dense_3_acc: 0.9841 - dense_3_acc_1: 0.9595 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8820 - dense_3_acc_4: 0.9444\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 6s 736us/step - loss: 0.7432 - dense_3_loss: 0.1881 - dense_3_acc: 0.9841 - dense_3_acc_1: 0.9646 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8887 - dense_3_acc_4: 0.9545\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 6s 781us/step - loss: 0.6834 - dense_3_loss: 0.1709 - dense_3_acc: 0.9863 - dense_3_acc_1: 0.9710 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.8944 - dense_3_acc_4: 0.9609\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 7s 851us/step - loss: 0.6282 - dense_3_loss: 0.1508 - dense_3_acc: 0.9880 - dense_3_acc_1: 0.9743 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9026 - dense_3_acc_4: 0.9661\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 7s 841us/step - loss: 0.5864 - dense_3_loss: 0.1406 - dense_3_acc: 0.9883 - dense_3_acc_1: 0.9774 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9081 - dense_3_acc_4: 0.9673\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 6s 782us/step - loss: 0.5440 - dense_3_loss: 0.1260 - dense_3_acc: 0.9905 - dense_3_acc_1: 0.9795 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9145 - dense_3_acc_4: 0.9720\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 6s 763us/step - loss: 0.5100 - dense_3_loss: 0.1151 - dense_3_acc: 0.9895 - dense_3_acc_1: 0.9819 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9204 - dense_3_acc_4: 0.9754\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 7s 832us/step - loss: 0.4768 - dense_3_loss: 0.1052 - dense_3_acc: 0.9915 - dense_3_acc_1: 0.9838 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9236 - dense_3_acc_4: 0.9801\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 7s 835us/step - loss: 0.4483 - dense_3_loss: 0.0964 - dense_3_acc: 0.9914 - dense_3_acc_1: 0.9830 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9294 - dense_3_acc_4: 0.9820\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 7s 874us/step - loss: 0.4230 - dense_3_loss: 0.0889 - dense_3_acc: 0.9909 - dense_3_acc_1: 0.9864 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9351 - dense_3_acc_4: 0.9849\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 7s 846us/step - loss: 0.4014 - dense_3_loss: 0.0836 - dense_3_acc: 0.9908 - dense_3_acc_1: 0.9866 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9375 - dense_3_acc_4: 0.9843\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 7s 827us/step - loss: 0.3795 - dense_3_loss: 0.0786 - dense_3_acc: 0.9918 - dense_3_acc_1: 0.9879 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9425 - dense_3_acc_4: 0.9874\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 6s 793us/step - loss: 0.3573 - dense_3_loss: 0.0713 - dense_3_acc: 0.9914 - dense_3_acc_1: 0.9883 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9436 - dense_3_acc_4: 0.9889\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 6s 790us/step - loss: 0.3372 - dense_3_loss: 0.0683 - dense_3_acc: 0.9925 - dense_3_acc_1: 0.9899 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9507 - dense_3_acc_4: 0.9879\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 6s 800us/step - loss: 0.3202 - dense_3_loss: 0.0635 - dense_3_acc: 0.9926 - dense_3_acc_1: 0.9899 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9534 - dense_3_acc_4: 0.9906\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 6s 764us/step - loss: 0.3037 - dense_3_loss: 0.0600 - dense_3_acc: 0.9930 - dense_3_acc_1: 0.9910 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9568 - dense_3_acc_4: 0.9914\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 6s 811us/step - loss: 0.2884 - dense_3_loss: 0.0566 - dense_3_acc: 0.9935 - dense_3_acc_1: 0.9914 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9591 - dense_3_acc_4: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb40a57d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time to train\n",
    "# It takes a few minutes on an quad-core CPU\n",
    "#输入的是Xoh，X的one-hot表示的矩阵\n",
    "model.fit([Xoh_train], outputs_train, epochs=30, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The final training loss should be in the range of 0.02 to 0.5\n",
    "\n",
    "The test loss should be at a similar level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2s 787us/step\n",
      "Test loss:  0.46567586398124694\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test performance\n",
    "outputs_test = list(Yoh_test.swapaxes(0,1))\n",
    "score = model.evaluate(Xoh_test, outputs_test) \n",
    "print('Test loss: ', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created this beautiful model, let's see how it does in action.\n",
    "\n",
    "The below code finds a random example and runs it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: t4.28am\n",
      "Tokenized: [32  7  2  5 11 14 25 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40]\n",
      "Prediction: [0, 4, 10, 2, 8]\n",
      "Prediction text: 04:28\n"
     ]
    }
   ],
   "source": [
    "# Let's visually check model output.\n",
    "import random as random\n",
    "\n",
    "i = random.randint(0, m)\n",
    "\n",
    "def get_prediction(model, x):\n",
    "    prediction = model.predict(x)\n",
    "    max_prediction = [y.argmax() for y in prediction]\n",
    "    str_prediction = \"\".join(ids_to_keys(max_prediction, machine_vocab))\n",
    "    return (max_prediction, str_prediction)\n",
    "\n",
    "max_prediction, str_prediction = get_prediction(model, Xoh[i:i+1])\n",
    "\n",
    "print(\"Input: \" + str(dataset[i][0]))\n",
    "print(\"Tokenized: \" + str(X[i]))\n",
    "print(\"Prediction: \" + str(max_prediction))\n",
    "print(\"Prediction text: \" + str(str_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, all introductions to Attention networks require a little tour.\n",
    "\n",
    "The below graph shows what inputs the model was focusing on when writing each individual letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x129.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = random.randint(0, m)\n",
    "\n",
    "def plot_attention_graph(model, x, Tx, Ty, human_vocab, layer=7):\n",
    "    # Process input\n",
    "    tokens = np.array([tokenize(x, human_vocab, Tx)])\n",
    "    tokens_oh = oh_2d(tokens, len(human_vocab))\n",
    "    \n",
    "    # Monitor model layer\n",
    "    layer = model.layers[layer]\n",
    "    \n",
    "    layer_over_time = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "    layer_output = layer_over_time([tokens_oh])\n",
    "    layer_output = [row.flatten().tolist() for row in layer_output]\n",
    "    \n",
    "    # Get model output\n",
    "    prediction = get_prediction(model, tokens_oh)[1]\n",
    "    \n",
    "    # Graph the data\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(1.8)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    plt.title(\"Attention Values per Timestep\")\n",
    "    \n",
    "    plt.rc('figure')\n",
    "    cax = plt.imshow(layer_output, vmin=0, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    plt.xlabel(\"Input\")\n",
    "    ax.set_xticks(range(Tx))\n",
    "    ax.set_xticklabels(x)\n",
    "    \n",
    "    plt.ylabel(\"Output\")\n",
    "    ax.set_yticks(range(Ty))\n",
    "    ax.set_yticklabels(prediction)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_attention_graph(model, dataset[i][0], Tx, Ty, human_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
